# æ·±åº¦å­¦ä¹ ä¸å¤§æ¨¡å‹ç²¾é€šä¹‹è·¯ (Deep Learning & LLM Mastery)

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)]()
[![Bilingual](https://img.shields.io/badge/Languages-EN%20%7C%20ä¸­æ–‡-blue.svg)]()

> **å…¨æ ˆå·¥ç¨‹æŒ‡å—ï¼šä»æœºå™¨å­¦ä¹ åŸºç¡€åˆ°ç”Ÿäº§çº§å¤§æ¨¡å‹ç³»ç»Ÿ**

è¿™æ˜¯ä¸€ä¸ªä¸ºå·¥ç¨‹å¸ˆã€ç ”ç©¶äººå‘˜å’ŒæŠ€æœ¯é¢†å¯¼è€…æ‰“é€ çš„ä¸“ä¸šçº§ã€é¢å‘ç”Ÿäº§çš„çŸ¥è¯†åº“ã€‚æœ¬ä»“åº“æä¾›äº†ä¸€æ¡ç»“æ„åŒ–çš„å­¦ä¹ è·¯å¾„ï¼Œæ¶µç›–ä»ç»å…¸æœºå™¨å­¦ä¹ åˆ°å‰æ²¿å¤§æ¨¡å‹ (LLM) å·¥ç¨‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) å’Œ MLOps çš„å®Œæ•´æŠ€æœ¯æ ˆã€‚

**ğŸŒ æ–‡æ¡£è¯­è¨€**: [**English**](README.md) | [**ä¸­æ–‡**](README_CN.md)

---

## ğŸ—ºï¸ å­¦ä¹ è·¯çº¿å›¾

è¯¾ç¨‹ä½“ç³»åˆ’åˆ†ä¸º **7 ä¸ªæ¸è¿›é˜¶æ®µ**ï¼Œæ—¨åœ¨å±‚å±‚é€’è¿›åœ°æ„å»ºæ ¸å¿ƒèƒ½åŠ›ã€‚

| é˜¶æ®µ | é¢†åŸŸ | æ ¸å¿ƒä¸»é¢˜ |
|------|------|----------|
| **01** | **åŸºç¡€ç¯‡ (Foundations)** | ç»å…¸ ML ç®—æ³•ã€æ·±åº¦å­¦ä¹ åŸºç¡€æ•°å­¦ |
| **02** | **ç¥ç»ç½‘ç»œ (Neural Networks)** | CNNã€åºåˆ—æ¨¡å‹ (RNN/LSTM)ã€ä¼˜åŒ–æŠ€æœ¯ |
| **03** | **NLP ä¸ Transformer** | æ³¨æ„åŠ›æœºåˆ¶ã€BERTã€GPTã€T5 æ¶æ„ |
| **04** | **å¤§æ¨¡å‹æ ¸å¿ƒ (LLM Core)** | é¢„è®­ç»ƒã€é«˜æ•ˆå¾®è°ƒ (PEFT)ã€å¯¹é½ (RLHF/DPO)ã€æç¤ºå·¥ç¨‹ã€æ¡†æ¶ä¸å·¥å…·ã€å¤šæ¨¡æ€ |
| **05** | **RAG ä¸ Agent** | å‘é‡æ•°æ®åº“ã€é«˜çº§ RAGã€Agent æ¨¡å¼ã€ç”Ÿäº§çº§åº”ç”¨ |
| **06** | **MLOps ä¸ç”Ÿäº§å·¥ç¨‹** | åˆ†å¸ƒå¼è®­ç»ƒã€æ¨¡å‹æœåŠ¡ã€ç›‘æ§è§‚æµ‹ã€è¯„æµ‹åŸºå‡†ã€åŸºç¡€è®¾æ–½éƒ¨ç½² |
| **07** | **å®æˆ˜é¡¹ç›® (Capstone)** | ç«¯åˆ°ç«¯ä¼ä¸šçº§ RAG ä¸å¾®è°ƒæµæ°´çº¿ |

---

## ğŸ“‚ çŸ¥è¯†åº“ç»“æ„

```
Daily-LLM/
â”‚
â”œâ”€â”€ 01-Foundations/               # ğŸŸ¢ Phase 1: åŸºçŸ³
â”‚   â”œâ”€â”€ machine-learning/         # ç®—æ³•åŸç†ã€æ•°å­¦åŸºç¡€ã€è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ deep-learning-basics/     # å¤šå±‚æ„ŸçŸ¥æœºã€åå‘ä¼ æ’­ã€æŸå¤±å‡½æ•°
â”‚
â”œâ”€â”€ 02-Neural-Networks/           # ğŸŸ¡ Phase 2: æ·±åº¦å­¦ä¹ æ¨¡å¼
â”‚   â”œâ”€â”€ cnn-architectures/        # è®¡ç®—æœºè§†è§‰æ¶æ„
â”‚   â”œâ”€â”€ sequence-models/          # åºåˆ—ä¸æ—¶é—´åºåˆ—å¤„ç†
â”‚   â”œâ”€â”€ training/                 # ç°ä»£è®­ç»ƒæŠ€å·§ä¸ä¼˜åŒ–
â”‚
â”œâ”€â”€ 03-NLP-Transformers/          # ğŸŸ  Phase 3: Transformer é©å‘½
â”‚   â”œâ”€â”€ attention-mechanisms/     # è‡ªæ³¨æ„åŠ›æœºåˆ¶æ·±åº¦è§£æ
â”‚   â”œâ”€â”€ transformer-architecture/ # ç¼–ç å™¨-è§£ç å™¨æ¶æ„
â”‚   â”œâ”€â”€ pretrained-models/        # é¢„è®­ç»ƒæ¨¡å‹å®¶æ— (BERT, GPT, T5)
â”‚
â”œâ”€â”€ 04-LLM-Core/                  # ğŸ”´ Phase 4: å¤§è¯­è¨€æ¨¡å‹
â”‚   â”œâ”€â”€ pre-training/             # æ•°æ®æµæ°´çº¿ã€Scaling Laws
â”‚   â”œâ”€â”€ peft/                     # å‚æ•°é«˜æ•ˆå¾®è°ƒ (LoRA/QLoRA)
â”‚   â”œâ”€â”€ alignment/                # å¯¹é½æŠ€æœ¯ (RLHF, DPO, å®‰å…¨æ€§)
â”‚   â”œâ”€â”€ prompt-engineering/       # æç¤ºå·¥ç¨‹ã€æ€ç»´é“¾ã€é«˜çº§æ¨¡å¼
â”‚   â”œâ”€â”€ frameworks/               # HuggingFaceã€LangChainã€LlamaIndexã€vLLM
â”‚   â”œâ”€â”€ multimodal/               # è§†è§‰è¯­è¨€æ¨¡å‹ (CLIP, LLaVA)
â”‚
â”œâ”€â”€ 05-RAG-Systems/               # ğŸŸ£ Phase 5: RAG ä¸ æ™ºèƒ½ä½“
â”‚   â”œâ”€â”€ rag-foundations/          # åˆ†å—ã€Embeddingã€é‡æ’åº (Rerank)
â”‚   â”œâ”€â”€ vector-databases/         # ç´¢å¼•ä¸æ£€ç´¢æŠ€æœ¯
â”‚   â”œâ”€â”€ agents/                   # ReActã€è§„åˆ’ã€å·¥å…·è°ƒç”¨
â”‚   â”œâ”€â”€ production/               # è¡Œä¸šåº”ç”¨åœºæ™¯ (ä»£ç åŠ©æ‰‹ã€æœç´¢ç­‰)
â”‚
â”œâ”€â”€ 06-MLOps-Production/          # ğŸ”µ Phase 6: å¤§è§„æ¨¡å·¥ç¨‹åŒ–
â”‚   â”œâ”€â”€ training-infrastructure/  # åˆ†å¸ƒå¼è®­ç»ƒ (FSDP/Deepspeed)
â”‚   â”œâ”€â”€ model-serving/            # æ¨ç†æœåŠ¡ (vLLM)ã€ä¼˜åŒ–ã€æ¨¡å‹ä»“åº“
â”‚   â”œâ”€â”€ monitoring/               # å¯è§‚æµ‹æ€§ã€æ¼‚ç§»æ£€æµ‹ã€è¯„ä¼°ã€è¯„æµ‹åŸºå‡†
â”‚   â”œâ”€â”€ deployment/               # K8sã€CI/CDã€æˆæœ¬ä¼˜åŒ–
â”‚
â””â”€â”€ 07-Capstone-Projects/         # âš« Phase 7: å®æˆ˜è½åœ°
    â”œâ”€â”€ enterprise-rag-system/    # ç”Ÿäº§çº§ RAG + Agent ç³»ç»Ÿ
    â”œâ”€â”€ finetune-deploy-pipeline/ # è‡ªåŠ¨åŒ–å¾®è°ƒä¸éƒ¨ç½²æµæ°´çº¿
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚
- **Python**: 3.8+
- **PyTorch**: 2.0+
- **ç¡¬ä»¶**: å­¦ä¹  LLM ç›¸å…³é˜¶æ®µå»ºè®®é…å¤‡æ”¯æŒ CUDA çš„ GPUã€‚

### å®‰è£…

```bash
git clone https://github.com/zkywsg/Daily-LLM.git
cd Daily-LLM

# å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt

# æˆ–æŒ‰å­¦ä¹ é˜¶æ®µé€‰æ‹©æ€§å®‰è£…:
# Phase 1-2: pip install torch numpy scikit-learn matplotlib
# Phase 3-4: pip install transformers datasets peft trl sentence-transformers
# Phase 5:   pip install sentence-transformers faiss-cpu chromadb langchain
# Phase 6-7: pip install vllm fastapi mlflow wandb
```

---

## ğŸ¯ ç›®æ ‡å—ä¼—

- **æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ**: ä»ä¼ ç»Ÿ ML å‘ LLM/GenAI è½¬å‹ã€‚
- **è½¯ä»¶å·¥ç¨‹å¸ˆ**: æ„å»ºåŸºäº AI çš„åº”ç”¨ (RAG/Agents)ã€‚
- **ç ”ç©¶äººå‘˜**: æ·±å…¥ç†è§£æŠ€æœ¯èƒŒåçš„åŸç†ã€‚
- **æŠ€æœ¯è´Ÿè´£äºº**: è®¾è®¡å¯æ‰©å±•çš„ AI åŸºç¡€è®¾æ–½ã€‚

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·é˜…è¯» [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£æˆ‘ä»¬çš„è¡Œä¸ºå‡†åˆ™å’Œæäº¤ Pull Request çš„æµç¨‹ã€‚

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚
