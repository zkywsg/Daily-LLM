# Deep Learning & LLM Mastery

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)]()
[![Bilingual](https://img.shields.io/badge/Languages-EN%20%7C%20ä¸­æ–‡-blue.svg)]()

> **The Complete Engineering Guide: From ML Fundamentals to Production LLM Systems**

A professional, production-focused knowledge base designed for engineers, researchers, and technical leaders. This repository provides a structured path from classical machine learning to cutting-edge Large Language Model (LLM) engineering, Retrieval-Augmented Generation (RAG), and MLOps.

**ğŸŒ Documentation**: [**English**](README_EN.md) | [**ä¸­æ–‡**](README.md)

---

## ğŸ—ºï¸ Learning Roadmap

The curriculum is organized into **7 Progressive Phases**, designed to build expertise layer by layer.

| Phase | Domain | Key Topics |
|-------|--------|------------|
| **01** | **Foundations** | Classical ML Algorithms, Deep Learning Basics |
| **02** | **Neural Networks** | CNNs, Sequence Models (RNN/LSTM), Optimization |
| **03** | **NLP & Transformers** | Attention Mechanisms, BERT, GPT, T5 Architecture |
| **04** | **LLM Core** | Pre-training, PEFT (LoRA), Alignment (RLHF/DPO), Prompt Engineering, Frameworks, Multimodal |
| **05** | **RAG & Agents** | Vector DBs, Advanced RAG, Agentic Patterns, Production Systems |
| **06** | **MLOps & Production** | Distributed Training, Serving, Monitoring, Benchmarks, Deployment Infrastructure |
| **07** | **Capstone Projects** | End-to-End Enterprise RAG & Fine-tuning Pipelines |

---

## ğŸ“‚ Repository Structure

```
Daily-LLM/
â”‚
â”œâ”€â”€ 01-Foundations/               # ğŸŸ¢ Phase 1: The Bedrock
â”‚   â”œâ”€â”€ machine-learning/         # Algorithms, Math, Evaluation
â”‚   â””â”€â”€ deep-learning-basics/     # MLP, Backprop, Loss Functions
â”‚
â”œâ”€â”€ 02-Neural-Networks/           # ğŸŸ¡ Phase 2: Deep Learning Patterns
â”‚   â”œâ”€â”€ cnn-architectures/        # Computer Vision Architectures
â”‚   â”œâ”€â”€ sequence-models/          # Sequence & Time-Series
â”‚   â””â”€â”€ training/                 # Modern Training Techniques
â”‚
â”œâ”€â”€ 03-NLP-Transformers/          # ğŸŸ  Phase 3: The Transformer Revolution
â”‚   â”œâ”€â”€ attention-mechanisms/     # Self-Attention Deep Dive
â”‚   â”œâ”€â”€ transformer-architecture/ # Encoder-Decoder, Positional Encoding
â”‚   â””â”€â”€ pretrained-models/        # Model Families (BERT, GPT, T5)
â”‚
â”œâ”€â”€ 04-LLM-Core/                  # ğŸ”´ Phase 4: Large Language Models
â”‚   â”œâ”€â”€ pre-training/             # Data Pipelines, Scaling Laws
â”‚   â”œâ”€â”€ peft/                     # Parameter-Efficient Fine-Tuning
â”‚   â”œâ”€â”€ alignment/                # RLHF, DPO, Safety
â”‚   â”œâ”€â”€ prompt-engineering/       # Prompt Design, CoT, Advanced Patterns
â”‚   â”œâ”€â”€ frameworks/               # HuggingFace, LangChain, LlamaIndex, vLLM
â”‚   â””â”€â”€ multimodal/               # Vision-Language Models (CLIP, LLaVA)
â”‚
â”œâ”€â”€ 05-RAG-Systems/               # ğŸŸ£ Phase 5: RAG & Agents
â”‚   â”œâ”€â”€ rag-foundations/          # Chunking, Embedding, Reranking
â”‚   â”œâ”€â”€ vector-databases/         # Indexing, Retrieval
â”‚   â”œâ”€â”€ agents/                   # ReAct, Planning, Tool Use
â”‚   â””â”€â”€ production/               # Industry Applications (Code, Search, etc.)
â”‚
â”œâ”€â”€ 06-MLOps-Production/          # ğŸ”µ Phase 6: Engineering at Scale
â”‚   â”œâ”€â”€ training-infrastructure/  # Distributed (FSDP/Deepspeed), Data Pipelines
â”‚   â”œâ”€â”€ model-serving/            # vLLM, Optimization, Registry
â”‚   â”œâ”€â”€ monitoring/               # Observability, Drift, Evaluation, Benchmarks
â”‚   â””â”€â”€ deployment/               # K8s, CI/CD, Cost Optimization
â”‚
â””â”€â”€ 07-Capstone-Projects/         # âš« Phase 7: Real-World Implementation
    â”œâ”€â”€ enterprise-rag-system/    # Production RAG with Agents
    â””â”€â”€ finetune-deploy-pipeline/ # Automated Fine-tuning Pipeline
```

---

## ğŸš€ Quick Start

### Prerequisites
- **Python**: 3.8+
- **PyTorch**: 2.0+
- **Hardware**: CUDA-capable GPU recommended for LLM phases.

### Installation

```bash
git clone https://github.com/zkywsg/Daily-LLM.git
cd Daily-LLM

# Install all dependencies
pip install -r requirements.txt

# Or install by learning phase:
# Phase 1-2: pip install torch numpy scikit-learn matplotlib
# Phase 3-4: pip install transformers datasets peft trl sentence-transformers
# Phase 5:   pip install sentence-transformers faiss-cpu chromadb langchain
# Phase 6-7: pip install vllm fastapi mlflow wandb
```

---

## ğŸ¯ Target Audience

- **ML Engineers**: Transitioning from classical ML to LLMs.
- **Software Engineers**: Building AI-powered applications (RAG/Agents).
- **Researchers**: Understanding the "Why" behind the "How".
- **Technical Leaders**: Designing scalable AI infrastructure.

---

## ğŸ¤ Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
