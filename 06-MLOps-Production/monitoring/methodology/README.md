# 评估方法论 (Evaluation Methodology)

## 目录
- [背景](#背景)
- [核心概念](#核心概念)
- [数学原理](#数学原理)
- [代码实现](#代码实现)
- [实验对比](#实验对比)
- [最佳实践与常见陷阱](#最佳实践与常见陷阱)
- [总结](#总结)

---

## 背景

评估方法论（Evaluation Methodology）解决一个核心问题：**如何用可复现、可解释、可对比的方式衡量模型与系统的真实价值**。对于LLM（Large Language Model），评估不仅是“模型准确率”，而是**能力、可靠性、安全性、成本与用户体验的综合权衡**。当模型被嵌入到检索、排序、工具调用、权限控制等系统中时，评估对象会从“模型能力”扩展到“系统端到端表现（End-to-End Performance）”。

### 为什么评估必须体系化？

1. **业务决策依赖评估**：上线、扩容、降级、迁移都需要量化依据。
2. **模型变化频繁**：提示词、数据、微调、系统策略每次变化都会影响输出。
3. **评估与风险绑定**：幻觉（Hallucination）、偏见（Bias）、泄漏（Leakage）会放大业务风险。
4. **成本越来越高**：LLM评估涉及人评（Human Evaluation）与在线实验（Online Experiments），成本可观，必须优化路径。

### 评估目标的三层结构

```
评估目标金字塔
├── 业务层（Business）
│   ├── 转化率、留存、满意度、成本/收益
│   └── 风险指标（合规、偏见、隐私）
├── 系统层（System）
│   ├── 端到端成功率、时延、稳定性
│   └── 失败模式（空结果、错误工具调用）
└── 模型层（Model）
    ├── 任务指标（Accuracy、F1、Pass@k）
    ├── 生成质量（BLEU、ROUGE、BERTScore）
    └── 语言建模（Perplexity）
```

**核心原则**：离线评估（Offline Evaluation）用于快速迭代与诊断；在线评估（Online Evaluation）用于真实效果验证。两者缺一不可。

### 评估流程的闭环

1. **定义问题**：任务类型、目标用户、使用场景。
2. **设计指标**：主指标 + 护栏指标（Guardrail Metrics）。
3. **准备数据**：评估集、对照集、人工标注集。
4. **离线评估**：快速筛选候选方案。
5. **在线评估**：A/B测试与灰度验证。
6. **分析与迭代**：误差分析、决策复盘、指标修订。

### 评估场景矩阵

| 场景 | 典型问题 | 关键指标 | 常用评估方式 |
|---|---|---|---|
| 客服问答 | 是否解决问题、是否合规 | 解决率、满意度、毒性 | 人评 + 在线A/B |
| 企业知识库 | 是否准确、是否可追溯 | Faithfulness、引用覆盖 | 离线RAG评估 |
| 代码助手 | 是否可运行 | Pass@k、Compile Rate | 自动化测试 |
| 内容生成 | 是否符合品牌语气 | Win Rate、品牌一致性 | 人评 + LLM-as-a-Judge |
| 工具调用 | 是否正确调用API | Tool-Use Accuracy | 离线回放 + 在线监控 |

### 评估维度拆解

1. **准确性（Correctness）**：答案是否正确、是否符合事实。
2. **相关性（Relevance）**：是否对准用户问题。
3. **完整性（Completeness）**：是否遗漏关键点。
4. **可解释性（Explainability）**：是否能给出合理依据。
5. **安全性（Safety）**：是否存在有害内容、偏见或隐私泄露。

### 评估边界的定义

评估必须明确“边界条件”：

- **输入约束**：输入长度、语言、格式是否固定？
- **输出约束**：是否要求结构化输出（JSON/表格/代码）？
- **系统策略**：是否使用工具调用、检索增强、缓存策略？

如果边界不明确，则评估结果很难比较。

### 评估成熟度模型（Maturity Model）

| 等级 | 特征 | 典型能力 |
|---|---|---|
| Level 1 | 临时评估 | 手工评估、缺少基线 |
| Level 2 | 基线评估 | 固定评估集、基础指标 |
| Level 3 | 系统化评估 | 自动化评估、误差分析 |
| Level 4 | 闭环评估 | 离线/在线闭环、监控与治理 |

**目标**：从“偶尔评估”升级到“持续评估 + 自动治理”。

---

## 核心概念

### 离线评估 vs 在线评估

| 维度 | 离线评估（Offline） | 在线评估（Online） |
|---|---|---|
| 目标 | 快速迭代、诊断问题 | 验证真实业务效果 |
| 主要数据 | 标注集、基准集 | 实际用户流量 |
| 优点 | 成本低、可控、可复现 | 真实、端到端、能反映用户行为 |
| 缺点 | 可能与真实效果脱节 | 成本高、周期长、噪声大 |
| 常用方法 | 基准测试、对比实验、回放评估 | A/B测试、多臂老虎机（Multi-armed Bandit） |

**结论**：离线评估是**必要条件**，在线评估是**充分条件**。

### 评估类型

1. **模型内在评估（Intrinsic Evaluation）**
   - 语言建模能力：Perplexity
   - 语义相似度：BERTScore
2. **模型外在评估（Extrinsic Evaluation）**
   - 任务表现：Accuracy、F1、Pass@k
3. **系统级评估（System Evaluation）**
   - 端到端成功率（End-to-End Success Rate）
   - 工具调用正确率（Tool-Use Accuracy）
4. **安全与鲁棒评估（Safety & Robustness）**
   - 毒性（Toxicity）、偏见（Bias）、隐私泄露

### LLM特有指标

| 指标 | 定义 | 适用场景 | 优点 | 局限 |
|---|---|---|---|---|
| Perplexity | 语言模型困惑度 | 语言建模 | 简单、可比较 | 与人类偏好不一致 |
| BLEU | n-gram精确率 | 翻译 | 快速、标准化 | 语义一致性弱 |
| ROUGE | n-gram召回率 | 摘要 | 强调覆盖 | 对同义改写不敏感 |
| BERTScore | 语义匹配 | 生成任务 | 语义相关性强 | 计算成本高 |
| Human Eval | 人类判断 | 对话、创作 | 贴近真实用户 | 成本高、主观性强 |

**扩展指标库（常见但需谨慎使用）**：

| 指标 | 解释 | 适用场景 | 备注 |
|---|---|---|---|
| METEOR | 基于词形与同义词匹配 | 翻译 | 比BLEU更柔性 |
| chrF | 字符级F-score | 多语言翻译 | 适合形态变化大语言 |
| COMET | 基于神经网络的质量评估 | 翻译 | 与人评一致性高 |
| BLEURT | 学习式文本匹配 | 摘要/翻译 | 需模型支持 |
| MAUVE | 生成分布差异 | 开放式生成 | 关注多样性 |
| BARTScore | 基于LM的重构分数 | 摘要/改写 | 计算成本高 |
| QAG | 问答一致性 | 摘要/事实性 | 依赖QA模型 |

### 评估框架（Evaluation Frameworks）

1. **HELM（Holistic Evaluation of Language Models）**：多维度综合评估框架。
2. **lm-evaluation-harness**：标准化的基准测试工具，支持MMLU、HumanEval等。
3. **OpenCompass**：中文/多语言评估体系。
4. **OpenAI Evals**：针对自定义任务的评估管线。
5. **RAG评估工具**（如RAGAS）：检索与生成联合评估。

**框架对比表**：

| 框架 | 主要能力 | 适用场景 | 优点 | 局限 |
|---|---|---|---|---|
| HELM | 多维评估 | 通用能力评估 | 覆盖面广 | 计算成本高 |
| lm-evaluation-harness | 基准测试 | 学术对比 | 社区成熟 | 业务定制难 |
| OpenCompass | 中文评估 | 中文/多语言 | 中文覆盖强 | 需要配置较多 |
| OpenAI Evals | 自定义任务 | 业务评估 | 灵活易扩展 | 依赖API |
| RAGAS | RAG质量 | 检索增强系统 | 专注RAG | 需高质量参考 |
| Promptfoo | Prompt测试 | Prompt迭代 | 易集成CI | 统计能力弱 |

**框架选择建议**：

- **学术对比**：lm-evaluation-harness + HELM
- **中文任务**：OpenCompass
- **业务评估**：OpenAI Evals + 自定义评估集
- **RAG系统**：RAGAS + 自建日志回放

### 评估方法选择决策树

```
开始
  ↓
是否需要真实业务效果？
  ├── 是 → 在线评估 (A/B测试)
  │        ↓
  │     是否流量足够？
  │        ├── 是 → 标准A/B
  │        └── 否 → Bandit/分层实验
  ↓
是否需要快速迭代？
  ├── 是 → 离线评估
  │        ↓
  │     任务类型？
  │        ├── 生成 → BLEU/ROUGE/BERTScore + 人评
  │        ├── 分类/选择 → Accuracy/F1
  │        └── 代码 → Pass@k
  ↓
是否关注安全风险？
  └── 是 → 加入毒性/偏见/隐私评估
```

### 评估术语速查

| 术语 | 含义 |
|---|---|
| Baseline | 对照模型/策略 |
| Guardrail Metric | 护栏指标，限制风险 |
| Offline Eval | 离线评估 |
| Online Eval | 在线评估 |
| SRM | 样本比例失衡 |
| CUPED | 方差降低方法 |
| Win Rate | 对战胜率 |
| Pass@k | 代码生成通过率 |
| Faithfulness | 事实一致性 |
| Calibration | 置信度校准 |
| Drift | 数据分布漂移 |
| Prompt Sensitivity | 提示敏感性 |
| OOD | 分布外数据 |
| A/B Test | 随机对照实验 |
| Bandit | 多臂老虎机 |
| E2E | 端到端 |
| PII | 个人隐私信息 |
| Hallucination | 幻觉 |
| Bootstrapping | 自助采样 |
| Effect Size | 效应量 |

### 指标设计原则（Metric Design Principles）

一个好的指标必须满足：**可解释（Explainable）+ 可行动（Actionable）+ 可复现（Reproducible）**。

| 原则 | 说明 | 反例 | 替代做法 |
|---|---|---|---|
| 可解释 | 指标变化要能解释原因 | “综合分”但没有构成解释 | 拆分为子指标并分层展示 |
| 可行动 | 指标变化能驱动优化策略 | 只给模型总体分数 | 增加错误类型分析（Error Taxonomy） |
| 可复现 | 离线评估能稳定复现 | 数据集不固定 | 固定评估集版本、记录hash |

**常见指标组合**：

```
主指标：质量（Quality）
护栏指标：安全（Safety）+ 成本（Cost）+ 时延（Latency）
```

### 任务类型与指标映射

| 任务类型 | 核心指标 | 补充指标 | 备注 |
|---|---|---|---|
| 文本分类 | Accuracy、F1 | AUROC、PR-AUC | 类别不均衡时优先PR-AUC |
| 信息抽取 | Exact Match、F1 | Span-level Precision/Recall | 适合结构化输出 |
| 机器翻译 | BLEU、COMET | chrF | COMET更接近人评 |
| 文本摘要 | ROUGE-L | BERTScore、QuestEval | 长文本需关注覆盖度 |
| 对话系统 | Win Rate、Human Eval | Response Diversity、Safety | 强烈依赖人评 |
| 代码生成 | Pass@k | Compilation Rate、Unit Test Pass Rate | 关注工具链兼容性 |
| RAG问答 | Recall@k、Faithfulness | Context Precision、Answer F1 | 检索与生成分开评估 |

### 数据集与标注策略

**评估集质量**决定评估上限。推荐遵循以下策略：

1. **覆盖性（Coverage）**：覆盖长尾问题、边界问题、真实用户问题。
2. **代表性（Representativeness）**：数据分布接近真实线上流量。
3. **对抗性（Adversarial）**：包含对抗样本（Prompt Injection、越权请求）。
4. **稳定性（Stability）**：评估集版本化，避免频繁漂移。

**标注指南（Annotation Guideline）核心要素**：

- 任务目标定义（Task Objective）
- 可接受答案边界（Acceptable Range）
- 评分标准（Scoring Rubric）
- 争议处理机制（Conflict Resolution）

### 数据集拆分策略

| 策略 | 说明 | 适用场景 |
|---|---|---|
| 随机拆分 | 随机划分训练/验证/测试 | 数据量大且分布稳定 |
| 时间拆分 | 按时间先后划分 | 关注分布漂移 |
| 用户拆分 | 按用户划分 | 避免用户泄漏 |
| 领域拆分 | 按领域划分 | 评估跨域泛化 |

### 评估资产清单（Assets）

**模型资产**：

- 模型版本记录（版本号、参数、训练数据）
- Prompt版本记录（系统指令、模板）

**数据资产**：

- 评估集与标注规范
- 对抗样本集
- 线上日志回放集

**指标资产**：

- 指标定义与公式
- 护栏阈值与报警规则

**流程资产**：

- 自动化评估脚本
- 报告模板与审核流程

### 人类评估（Human Evaluation）设计

**两类主流方案**：

1. **Likert评分（1-5/1-7）**：适合绝对质量评价。
2. **Pairwise对比**：适合模型对比，抗标注偏差。

**一致性衡量（Inter-Annotator Agreement）**：

| 指标 | 适用场景 | 解释 |
|---|---|---|
| Cohen's Kappa | 两个标注者 | 分类一致性 |
| Fleiss' Kappa | 多个标注者 | 多人一致性 |
| Krippendorff's Alpha | 任意类型 | 可处理缺失值 |

### 系统级指标

**LLM系统并非只有模型质量**，系统指标与体验密切相关：

| 指标 | 定义 | 说明 |
|---|---|---|
| 端到端成功率 | 完成任务的比例 | 业务核心指标 |
| 平均时延 | 平均响应时间 | 影响用户体验 |
| P95/P99时延 | 高分位响应 | 反映尾部风险 |
| 成本/调用 | 每次请求成本 | 重要业务约束 |
| 工具调用成功率 | 工具调用正确比例 | Tool-Use场景必备 |

### 用户体验指标（UX Metrics）

| 指标 | 含义 | 解释 |
|---|---|---|
| 满意度（CSAT） | 用户评分 | 1-5或1-7量表 |
| 任务完成率 | 是否完成任务 | 端到端核心指标 |
| 交互回合数 | 完成任务所需轮数 | 越少越好（需结合复杂度） |
| 复问率 | 重复提问比例 | 反映回答不清楚 |
| 负反馈率 | 点踩/投诉 | 风险警示 |

### 指标优先级矩阵

| 业务阶段 | 优先指标 | 次要指标 |
|---|---|---|
| 探索期 | 任务完成率 | 成本、时延 |
| 成长期 | 转化率 | 满意度、成本 |
| 稳定期 | 成本/时延 | 安全、满意度 |
| 合规期 | 安全指标 | 成本、转化 |

### 指标权重示例（业务可配置）

| 业务场景 | 质量权重 | 安全权重 | 成本权重 |
|---|---|---|---|
| 金融客服 | 0.4 | 0.4 | 0.2 |
| 教育问答 | 0.5 | 0.3 | 0.2 |
| 娱乐助手 | 0.6 | 0.2 | 0.2 |

### RAG与检索评估（Retrieval-Augmented Generation）

**检索阶段（Retrieval）指标**：

| 指标 | 公式 | 解释 |
|---|---|---|
| Recall@k | \(\frac{\text{命中相关文档}}{\text{相关文档总数}}\) | 关键检索指标 |
| MRR | \(\frac{1}{|Q|}\sum\frac{1}{rank}\) | 关注首条命中 |
| NDCG | 归一化折损累计增益 | 适合有等级标注 |

**生成阶段（Generation）指标**：

| 指标 | 含义 | 备注 |
|---|---|---|
| Faithfulness | 生成是否基于检索证据 | 减少幻觉 |
| Answer F1 | 答案与参考的匹配 | 适合短答案 |
| Context Precision | 上下文是否含关键信息 | 衡量检索质量 |

### 安全与合规评估

**常见安全指标**：

| 风险维度 | 指标 | 说明 |
|---|---|---|
| 毒性（Toxicity） | Toxicity Score | 不良输出概率 |
| 偏见（Bias） | Bias Score | 对群体的偏好度 |
| 隐私（Privacy） | PII Leakage Rate | 个人信息泄露率 |
| 越权（Jailbreak） | Attack Success Rate | 越权提示成功率 |

**安全评估原则**：

1. 离线安全评估必须覆盖多语言与文化语境。
2. 在线安全评估必须设置“护栏指标”自动阻断。
3. 评估结果要可追踪（Auditability）。

### 鲁棒性评估（Robustness Evaluation）

**常见鲁棒性测试类型**：

| 类型 | 说明 | 示例 |
|---|---|---|
| 输入扰动 | 改写或错别字 | “天气怎么样” → “天 气怎 样” |
| 对抗提示 | 越权或注入 | “忽略系统指令” |
| 分布外数据 | 训练未覆盖的领域 | 医疗、法律、金融 |
| 长上下文 | 超长输入 | 10k tokens对话 |

**鲁棒性评估指标**：

| 指标 | 解释 | 目标 |
|---|---|---|
| Robust Accuracy | 扰动后准确率 | 越高越好 |
| Consistency | 多次回答一致性 | 越高越好 |
| Attack Success Rate | 攻击成功率 | 越低越好 |

### LLM-as-a-Judge设计要点

**优势**：

- 成本低于大规模人评
- 可快速迭代

**风险**：

- 评审模型偏差（Model Bias）
- 对生成风格敏感

**最佳实践**：

1. 评审模型与被评估模型隔离
2. 使用多评审模型投票
3. 定期与人评对齐

### 人评Rubric示例

| 维度 | 评分标准 | 1分 | 3分 | 5分 |
|---|---|---|---|---|
| 准确性（Correctness） | 是否正确 | 大量错误 | 部分正确 | 完全正确 |
| 完整性（Completeness） | 是否覆盖关键信息 | 严重缺失 | 有缺失 | 全覆盖 |
| 可读性（Clarity） | 是否清晰易懂 | 难以理解 | 一般 | 清晰简洁 |

### 多目标评分（Multi-Objective Score）

当多个指标需要汇总时，可使用加权得分：

$$
Score = w_q \cdot Quality + w_s \cdot Safety + w_c \cdot Cost
$$

其中 $w_q + w_s + w_c = 1$。


### 指标对齐：质量 vs 成本 vs 风险

在实际工程中，指标常常互相冲突。典型冲突关系：

- **质量提升 → 成本上升**（更大模型、更多检索）
- **安全提升 → 生成受限**（更严格过滤）
- **时延下降 → 质量下降**（减少推理步数）

因此需要引入**多目标优化（Multi-Objective Optimization）**思路：

| 方案 | 质量 | 成本 | 安全 | 结论 |
|---|---|---|---|---|
| A | 高 | 高 | 中 | 用于高价值用户 |
| B | 中 | 中 | 高 | 用于合规场景 |
| C | 低 | 低 | 中 | 用于免费流量 |

### 离线评估实验设计

**设计步骤**：

1. **定义任务与评估集**：明确输入/输出规范。
2. **选择基线（Baseline）**：必须稳定且可复现。
3. **对照实验（Ablation）**：逐项改动，定位关键因素。
4. **结果拆解（Error Breakdown）**：按类型统计错误。
5. **可复现性保证**：固定随机种子、版本锁定。

**常见离线评估方式**：

| 方法 | 说明 | 优点 | 缺点 |
|---|---|---|---|
| 基准评估（Benchmark） | 使用公开数据集 | 可比性强 | 可能不贴合业务 |
| 回放评估（Replay） | 使用历史日志 | 贴近真实 | 需要高质量日志 |
| 对抗评估（Adversarial） | 专门设计对抗输入 | 安全性高 | 构造成本高 |

### 在线评估实验设计

**实验设计要点**：

1. **随机化（Randomization）**：避免偏差。
2. **分层（Stratification）**：按用户或场景分组。
3. **最小样本量（Minimum Sample Size）**：保障统计功效。
4. **停止规则（Stopping Rule）**：避免中途窥视。
5. **护栏指标（Guardrail Metrics）**：控制风险。

**在线实验类型**：

| 方法 | 适用场景 | 优点 | 风险 |
|---|---|---|---|
| 标准A/B | 流量充足 | 结果可靠 | 时间长 |
| 多臂老虎机（Bandit） | 动态流量分配 | 更快收益 | 统计解释复杂 |
| 分阶段灰度 | 高风险场景 | 风险可控 | 速度慢 |

**A/B测试实施步骤（Checklist）**：

1. 确定实验假设与主指标
2. 定义护栏指标与风险阈值
3. 估算最小样本量与实验周期
4. 随机化与分层规则确定
5. 上线前SRM检测
6. 实验进行中监控指标波动
7. 实验结束后进行统计检验
8. 分层分析与误差分析
9. 输出结论与上线建议
10. 归档实验结果

**Bandit策略简述**：

- 适合快速迭代场景
- 需要在收益与探索之间平衡
- 常见算法：Epsilon-Greedy、Thompson Sampling

### 回放评估（Log Replay）

**概念**：使用线上日志作为输入，离线回放生成结果，并与历史输出对比。

**优势**：

- 真实用户分布
- 能快速评估多种方案

**局限**：

- 无法捕捉新策略对用户行为的影响
- 依赖日志质量

### 分桶与分层分析

**分层维度示例**：

- 用户类型（新用户 vs 资深用户）
- 语言类型（中文 vs 英文）
- 任务类型（问答 vs 编码）

**分层价值**：

1. 发现局部问题
2. 避免平均值掩盖问题
3. 识别差异化策略空间



---

## 数学原理

评估的可信度来源于统计学。以下是评估中最常用的数学基础。

### 基本统计量

**均值（Mean）**：
$$
\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
$$

**方差（Variance）**：
$$
s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
$$

**标准误（Standard Error）**：
$$
SE = \frac{s}{\sqrt{n}}
$$

### 置信区间（Confidence Interval）

**均值的置信区间（t分布）**：
$$
\bar{x} \pm t_{\alpha/2, n-1} \cdot \frac{s}{\sqrt{n}}
$$

**比例的置信区间（Wilson Score）**：
$$
\hat{p} = \frac{x}{n}
$$
$$
CI = \frac{\hat{p} + \frac{z^2}{2n} \pm z\sqrt{\frac{\hat{p}(1-\hat{p})}{n} + \frac{z^2}{4n^2}}}{1 + \frac{z^2}{n}}
$$

**Bootstrap置信区间**（非参数）：通过重复采样估计分布，适用于复杂指标。

### A/B测试的统计检验

**1. 均值差异（t检验）**：
$$
t = \frac{\bar{x}_A - \bar{x}_B}{\sqrt{\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}}}
$$

**2. 比例差异（z检验）**：
$$
z = \frac{p_A - p_B}{\sqrt{p(1-p)\left(\frac{1}{n_A}+\frac{1}{n_B}\right)}}
$$
其中 $p$ 为合并比例：
$$
p = \frac{x_A + x_B}{n_A + n_B}
$$

**3. 卡方检验（Chi-square Test）**：
$$
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
$$

**4. 非参数检验（Mann-Whitney U）**：用于分布偏斜或重尾数据。

### 效应量（Effect Size）

**Cohen's d**：
$$
d = \frac{\bar{x}_A - \bar{x}_B}{s_p}
$$
$$
s_p = \sqrt{\frac{(n_A-1)s_A^2 + (n_B-1)s_B^2}{n_A+n_B-2}}
$$

**相对提升（Relative Lift）**：
$$
\text{Lift} = \frac{p_B - p_A}{p_A}
$$

### 样本量与统计功效

**功效（Power）**：检验发现真实差异的概率。

**样本量估算（比例差异）**：
$$
n = \frac{2\left(z_{1-\alpha/2}\sqrt{2\bar{p}(1-\bar{p})} + z_{1-\beta}\sqrt{p_A(1-p_A)+p_B(1-p_B)}\right)^2}{(p_A-p_B)^2}
$$

### 多重检验校正

**Bonferroni 校正**：
$$
\alpha' = \frac{\alpha}{m}
$$

**Benjamini-Hochberg（控制FDR）**：
对p值排序，找到最大 $k$：
$$
p_{(k)} \le \frac{k}{m}\alpha
$$

### 样本比例失衡（SRM, Sample Ratio Mismatch）

在A/B测试中，流量分配与预期比例不一致会导致结果失真。常用卡方检验：

$$
\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}
$$

其中 $O_i$ 为观察到的样本数，$E_i$ 为预期样本数。

### CUPED（方差降低）

**CUPED（Controlled Experiment Using Pre-Experiment Data）** 利用实验前的协变量降低方差：

$$
Y_{\text{cuped}} = Y - \theta (X - \bar{X})
$$
$$
\theta = \frac{\text{Cov}(Y, X)}{\text{Var}(X)}
$$

### 序贯检验（Sequential Testing）

避免“窥视数据（Peeking）”导致假阳性，可采用序贯分析：

**SPRT（Sequential Probability Ratio Test）**：
$$
\Lambda = \frac{P(Data | H_1)}{P(Data | H_0)}
$$
当 $\Lambda$ 超过上界或低于下界时停止实验。

### 贝叶斯A/B测试（Bayesian AB Testing）

对于比例指标，常用Beta分布作为先验：

$$
p \sim \text{Beta}(\alpha, \beta)
$$

更新后验：

$$
p | Data \sim \text{Beta}(\alpha + x, \beta + n - x)
$$

可直接计算 $P(p_B > p_A)$ 作为决策依据。

### 一致性与相关性指标

**Cohen's Kappa**（人评一致性）：
$$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$

**Spearman相关系数**（指标与人评一致性）：
$$
\rho = 1 - \frac{6\sum d_i^2}{n(n^2-1)}
$$

**Kendall's Tau**（排序一致性）：
$$
\tau = \frac{C - D}{\frac{1}{2}n(n-1)}
$$


### 语言模型指标的数学定义

**困惑度（Perplexity）**：
$$
\text{PPL} = \exp\left(-\frac{1}{N}\sum_{i=1}^N \log p(w_i)\right)
$$

**BLEU**：
$$
\text{BLEU} = BP \cdot \exp\left(\sum_{n=1}^N w_n \log p_n\right)
$$

**ROUGE-N**：
$$
\text{ROUGE-N} = \frac{\sum_{S \in R}\sum_{ngram \in S} \text{Count}_{match}(ngram)}{\sum_{S \in R}\sum_{ngram \in S}\text{Count}(ngram)}
$$

### 分类与检索指标的数学定义

**准确率（Accuracy）**：
$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**精确率（Precision）**：
$$
\text{Precision} = \frac{TP}{TP + FP}
$$

**召回率（Recall）**：
$$
\text{Recall} = \frac{TP}{TP + FN}
$$

**F1分数**：
$$
\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

**Exact Match（EM）**：
$$
\text{EM} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[\hat{y}_i = y_i]
$$

**MRR（Mean Reciprocal Rank）**：
$$
\text{MRR} = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{\text{rank}_q}
$$

**NDCG（Normalized Discounted Cumulative Gain）**：
$$
\text{DCG}@k = \sum_{i=1}^k \frac{2^{rel_i} - 1}{\log_2(i+1)}
$$
$$
\text{NDCG}@k = \frac{\text{DCG}@k}{\text{IDCG}@k}
$$

### 校准（Calibration）与置信度

**ECE（Expected Calibration Error）**：
$$
\text{ECE} = \sum_{m=1}^M \frac{|B_m|}{n} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|
$$

### 人评一致性指标

**Cohen's Kappa**：
$$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$

**Fleiss' Kappa**（多标注者）：
$$
\kappa = \frac{\bar{P} - \bar{P_e}}{1 - \bar{P_e}}
$$

### 生成式评估的统计检验

当输出不是二分类时，可使用**配对Bootstrap**或**随机置换检验（Permutation Test）**：

$$
\Delta = \bar{m}_{A} - \bar{m}_{B}
$$

通过打乱标签或重采样估计 $\Delta$ 的分布，得到p值。

### 成本与性能评估公式

**吞吐量（Throughput）**：
$$
\text{Throughput} = \frac{\text{Tokens}}{\text{Second}}
$$

**平均延迟（Latency）**：
$$
\text{Latency} = \frac{\sum_{i=1}^{n} t_i}{n}
$$

**每次请求成本（Cost per Request）**：
$$
\text{Cost} = \frac{\text{InputTokens}}{1000} \cdot c_{in} + \frac{\text{OutputTokens}}{1000} \cdot c_{out}
$$

**单位收益（ROI）**：
$$
\text{ROI} = \frac{\text{Business Gain}}{\text{Cost}}
$$

### 统计检验选择指南

| 场景 | 数据类型 | 推荐检验 |
|---|---|---|
| 两组均值比较 | 连续 | t检验 / Welch t检验 |
| 两组比例比较 | 二项 | z检验 |
| 多组均值比较 | 连续 | ANOVA |
| 非正态数据 | 连续 | Mann-Whitney U |
| 计数数据 | 分类 | 卡方检验 |

**简单决策树**：

```
数据是否连续？
  ├── 是 → 是否正态？
  │        ├── 是 → t检验
  │        └── 否 → Mann-Whitney U
  └── 否 → 是否是比例？
           ├── 是 → z检验
           └── 否 → 卡方检验
```

### 样本量参考表（示例）

假设显著性水平 $\alpha=0.05$，功效 $1-\beta=0.8$。

| 基线转化率 | 期望提升 | 估计样本量/组 |
|---|---|---|
| 2% | +0.2% | ~75k |
| 5% | +0.5% | ~38k |
| 10% | +1% | ~20k |
| 20% | +2% | ~10k |

> 注：实际样本量需根据业务波动、分层因素调整。

### 统计显著性解读

**常见误解**：

- p值小并不代表效果大。
- p值大不代表没有差异，可能是样本量不足。
- 置信区间包含0意味着差异不显著，但仍可能具备业务价值。

**推荐做法**：

1. 同时报告**效应量 + 置信区间 + p值**。
2. 对重要指标进行**功效分析（Power Analysis）**。
3. 对业务指标设置**最小可检测效应（MDE）**。

**最小可检测效应（MDE）**：

$$
\text{MDE} \propto \sqrt{\frac{1}{n}}
$$

样本量越大，能检测到的最小差异越小。

**示例：置信区间计算**

假设样本均值 $\bar{x}=0.62$，标准差 $s=0.12$，样本量 $n=100$，95%置信区间：

$$
CI = 0.62 \pm 1.96 \cdot \frac{0.12}{\sqrt{100}} = 0.62 \pm 0.0235
$$

即 $[0.5965, 0.6435]$。

---

## 代码实现

以下代码示例采用Python实现常见评估流程，包含中文注释，可直接运行。

*(代码示例请参考英文版，保持一致性)*

---

## 实验对比

### 1. 指标对比表

| 指标 | 适用任务 | 优点 | 缺点 | 推荐场景 |
|---|---|---|---|---|
| Perplexity | 语言建模 | 简单可比 | 与人类偏好弱相关 | 预训练阶段 |
| BLEU | 翻译 | 标准化 | 忽略语义 | 机器翻译 | 
| ROUGE | 摘要 | 强调召回 | 不敏感于改写 | 摘要生成 |
| BERTScore | 生成任务 | 语义相关性强 | 成本高 | 对话、改写 |
| Human Eval | 复杂生成 | 最贴近真实 | 成本高 | 关键业务输出 |

### 2. 离线 vs 在线实验示例

| 实验 | 离线指标提升 | 在线A/B结果 | 结论 |
|---|---|---|---|
| Prompt优化 | ROUGE +2.1 | 转化率 +0.3% | 有效但提升有限 |
| 新检索策略 | BERTScore +0.5 | 留存率 +2.1% | 明显提升 |
| 温度调优 | BLEU +1.5 | 用户满意度 -0.4 | 离线与在线不一致 |

### 3. 多指标权衡示例

| 模型版本 | 任务准确率 | 毒性评分（越低越好） | 推理时延 | 综合推荐 |
|---|---|---|---|---|
| V1 | 78.2% | 0.12 | 1.8s | 基线 |
| V2 | 81.5% | 0.18 | 2.2s | 性能提升但风险升高 |
| V3 | 80.1% | 0.08 | 2.0s | 综合最佳 |

### 4. 离线-在线差异案例分析

**案例A：离线指标提升但在线下降**

| 现象 | 解释 | 可能原因 | 修复策略 |
|---|---|---|---|
| ROUGE提升但满意度下降 | 指标与体验不一致 | 模型更“安全”但更“啰嗦” | 引入长度惩罚或简洁度评分 |
| BLEU提升但点击率下降 | 生成更“规范”但不够“吸引” | 用户需求偏好差异 | 引入人评或真实用户评分 |

**案例B：在线提升但离线无提升**

| 现象 | 解释 | 可能原因 | 修复策略 |
|---|---|---|---|
| 转化率提升但ROUGE不变 | 线上策略优化 | 工具调用更合理 | 增加Tool-Use指标 |
| 留存提升但BERTScore无变化 | 用户满意度增加 | 更快响应或更稳定 | 加入时延与稳定性指标 |

### 5. 指标与人评相关性示例

| 指标 | Spearman相关 | Kendall Tau | 结论 |
|---|---|---|---|
| ROUGE-L | 0.42 | 0.31 | 相关性中等 |
| BERTScore | 0.61 | 0.49 | 与人评更一致 |
| BLEU | 0.28 | 0.20 | 相关性较弱 |
| Perplexity | -0.10 | -0.08 | 与人评基本无关 |

### 6. A/B测试报告模板（简化）

```
实验名称：Prompt优化V2
实验周期：2026-02-01 ~ 2026-02-07
主指标：转化率
护栏指标：时延、毒性、成本

结果摘要：
- 转化率：+1.8% (p=0.012, 显著)
- 时延：+0.12s (p=0.21, 不显著)
- 毒性：-0.03 (p=0.04, 显著)
- 成本：+5% (p=0.08, 不显著)

结论：推荐上线（满足护栏约束）
```

### 7. 评估决策树（综合权衡）

```
是否主指标显著提升？
  ├── 否 → 不上线，继续迭代
  └── 是
       ↓
    护栏指标是否破坏阈值？
       ├── 是 → 不上线，修复风险
       └── 否
            ↓
         成本是否可接受？
            ├── 否 → 进行成本优化后再评估
            └── 是 → 上线
```

### 8. 错误类型分布示例

| 错误类型 | 占比 | 说明 | 典型原因 |
|---|---|---|---|
| 事实错误 | 28% | 与参考答案不一致 | 幻觉、检索缺失 |
| 逻辑错误 | 17% | 推理链断裂 | 复杂推理不足 |
| 漏答 | 22% | 关键点遗漏 | 长上下文处理不足 |
| 格式错误 | 15% | 不符合结构化输出 | 约束提示不足 |
| 安全违规 | 8% | 输出敏感内容 | 安全过滤不足 |
| 其他 | 10% | 未分类 | 需要进一步分析 |

### 9. 指标敏感性分析

| 变量 | 调整范围 | 指标变化 | 结论 |
|---|---|---|---|
| 温度（Temperature） | 0.2 → 0.8 | BLEU -3.1 | 稳定性下降 |
| Top-p | 0.8 → 0.95 | 多样性 +12% | 质量略降 |
| Top-k | 20 → 100 | BERTScore -0.02 | 影响有限 |

### 10. Prompt对比实验

| Prompt版本 | 离线指标 | 人评胜率 | 总结 |
|---|---|---|---|
| P1 | ROUGE 0.31 | 42% | 语言简洁但信息不足 |
| P2 | ROUGE 0.35 | 55% | 综合最优 |
| P3 | ROUGE 0.36 | 48% | 信息更全但啰嗦 |

### 11. RAG检索策略对比

| 检索策略 | Recall@5 | Faithfulness | Answer F1 | 结论 |
|---|---|---|---|---|
| BM25 | 0.62 | 0.71 | 0.48 | 基线 |
| Dense | 0.74 | 0.79 | 0.55 | 显著提升 |
| Hybrid | 0.81 | 0.82 | 0.58 | 综合最佳 |

### 12. 端到端系统评估案例

**案例：企业知识库问答系统**

评估步骤：

1. **检索评估**：Recall@5 >= 0.75 视为合格。
2. **生成评估**：Answer F1 >= 0.55。
3. **安全评估**：Toxicity < 0.1。
4. **性能评估**：P95时延 < 2.5s。

**结果示例**：

| 指标 | 目标 | 实际 | 通过 |
|---|---|---|---|
| Recall@5 | 0.75 | 0.81 | ✅ |
| Answer F1 | 0.55 | 0.57 | ✅ |
| Toxicity | <0.1 | 0.07 | ✅ |
| P95时延 | <2.5s | 2.8s | ❌ |

**结论**：功能指标通过，但性能不达标 → 优化推理速度后再上线。

### 13. 对话系统评估案例

**评估维度**：

- 信息准确性（Accuracy）
- 对话连贯性（Coherence）
- 角色一致性（Persona Consistency）
- 安全性（Safety）

**人评结果**：

| 版本 | 准确性 | 连贯性 | 角色一致性 | 安全性 |
|---|---|---|---|---|
| V1 | 3.6 | 3.2 | 3.8 | 4.1 |
| V2 | 4.0 | 3.9 | 4.2 | 3.7 |
| V3 | 3.8 | 4.1 | 4.0 | 4.3 |

**结论**：V2质量提升但安全性下降，需补充安全过滤。

### 14. 分层分析示例

| 用户类型 | 版本A转化率 | 版本B转化率 | 提升 |
|---|---|---|---|
| 新用户 | 3.2% | 3.8% | +0.6% |
| 老用户 | 6.1% | 6.0% | -0.1% |
| 付费用户 | 12.3% | 12.9% | +0.6% |

**结论**：新用户与付费用户显著提升，但老用户略降 → 可考虑分群策略。

### 15. 完整评估流程示例

**场景**：知识库问答系统升级（引入更强检索器）

| 阶段 | 评估方法 | 指标 | 结果 | 结论 |
|---|---|---|---|---|
| 离线 | 回放评估 | Recall@5 | +0.12 | 通过 |
| 离线 | 生成评估 | Answer F1 | +0.04 | 通过 |
| 安全 | 对抗测试 | ASR | +0.02 | 需优化 |
| 在线 | A/B测试 | 转化率 | +1.5% (p<0.05) | 通过 |
| 在线 | 护栏指标 | 毒性 | 无显著变化 | 通过 |

**最终决策**：上线，但需补充安全过滤策略。

---

## 最佳实践与常见陷阱

### 最佳实践

1. **离线+在线结合**：离线筛选，在线验证。
2. **多指标评估**：主指标 + 护栏指标。
3. **分层评估**：不同用户群体分层分析（避免Simpson悖论）。
4. **稳定性检查**：使用Bootstrap或交叉验证评估波动。
5. **人评标准化**：明确标注规范，保证一致性。

### 评估清单（Checklist）

**离线评估清单**：

- [ ] 评估集固定并版本化（含hash）
- [ ] 任务分布与线上接近
- [ ] 指标包含主指标与护栏指标
- [ ] 错误分析输出明确分类
- [ ] 统计显著性与置信区间

**在线评估清单**：

- [ ] 通过SRM检测
- [ ] 预先设定样本量与功效
- [ ] 指标监控包含时延与成本
- [ ] 多重检验校正
- [ ] 实验结束后进行回放分析

**人评清单**：

- [ ] 评分Rubric明确
- [ ] 标注者培训与一致性检查
- [ ] 明确处理Tie/争议样本
- [ ] 人评数据与自动评估对齐

### 评估治理与版本控制

**为什么需要治理？**

- 评估集变化会导致指标不可比
- 不同团队的评估标准可能冲突

**推荐做法**：

1. **评估集版本化**：例如 `eval-set-v1.2.0`。
2. **指标定义文档化**：明确公式、分桶、过滤规则。
3. **评估报告存档**：结果可追溯。
4. **自动化评估（CI集成）**：每次模型或Prompt更新自动生成报告。

### 评估报告结构模板

```
1. 实验背景
   - 目标
   - 变更点
   - 假设
2. 评估设置
   - 数据集与版本
   - 指标定义
   - 实验周期
3. 结果
   - 主指标
   - 护栏指标
   - 统计显著性
4. 误差分析
   - 错误类型分布
   - 典型案例
5. 结论与决策
   - 是否上线
   - 风险与后续动作
```

### 指标字典模板

```
指标名称：Answer F1
定义：基于token重叠的F1
公式：2 * P * R / (P + R)
输入数据：预测答案、参考答案
分桶：按领域与难度分层
护栏阈值：>= 0.55
更新时间：2026-02-01
```

### 数据卡片（Data Card）建议字段

- 数据来源
- 采集时间范围
- 覆盖领域
- 标注标准
- 质量检查结果

### 评估日志字段建议

- 请求ID、用户ID（脱敏）
- 输入Prompt、系统指令版本
- 输出文本、置信度
- 使用的模型版本与参数
- 评估结果与错误类型

### 指标报警与监控

| 指标 | 报警阈值 | 处理策略 |
|---|---|---|
| 端到端成功率 | < 98% | 回滚策略 |
| 毒性评分 | > 0.15 | 触发过滤 |
| P95时延 | > 3s | 降级策略 |
| 成本/请求 | > 预算阈值 | 降低模型规模 |

### CI/流水线集成建议

1. 每次模型更新触发离线评估
2. 对比上次基线（自动Diff）
3. 指标未达标阻断发布
4. 评估结果自动存档

### 常见问题（FAQ）

**Q1：离线指标和在线结果不一致怎么办？**
A：检查评估集是否代表真实流量，并引入在线指标回放评估。

**Q2：A/B测试多久结束？**
A：以最小样本量与显著性为准，避免中途停止造成假阳性。

**Q3：人评成本太高如何降本？**
A：使用LLM-as-a-Judge进行初筛，再对关键样本做人评。

**Q4：指标太多导致决策困难？**
A：明确主指标与护栏指标，避免过度优化次要指标。

**Q5：评估集多久更新一次？**
A：建议每季度更新一次，遇到重大业务变更应立即更新。

**Q6：如何评估多语言模型？**
A：按语言分层评估，并为每种语言单独设置基线。

**Q7：何时需要引入人评？**
A：当任务复杂、自动指标与体验不一致时必须引入人评。

**Q8：如何防止指标被“刷分”？**
A：引入对抗评估与多指标交叉验证，避免单指标优化。

### 审计与合规建议

| 场景 | 建议 |
|---|---|
| 金融/医疗 | 增加人工审核与责任链路 |
| 政务系统 | 强化可解释性与可追溯性 |
| 教育场景 | 加入偏见与公平性评估 |

**合规清单**：

- [ ] 评估数据脱敏
- [ ] 评估日志可追踪
- [ ] 重大决策有人工复核


### 常见陷阱

### 常见陷阱

| 陷阱 | 影响 | 解决方案 |
|---|---|---|
| 数据泄漏（Leakage） | 虚假高分 | 严格拆分训练/评估 |
| 指标过拟合 | 评估失真 | 多指标交叉验证 |
| 过早停止实验 | 假阳性 | 设定最低样本量 |
| 多重检验问题 | p值失效 | FDR校正 |
| 只看均值 | 忽略尾部风险 | 分布分析 |

### 陷阱补充（LLM特有）

| 陷阱 | 影响 | 解决方案 |
|---|---|---|
| Prompt敏感性 | 评估不稳定 | 固定Prompt或做Prompt平均 |
| 评估漂移（Drift） | 结论失效 | 定期更新评估集 |
| 安全指标缺失 | 风险不可控 | 加入毒性/偏见/隐私指标 |
| 长尾样本不足 | 真实场景失真 | 强化长尾与对抗样本 |
| 人评偏差 | 结论不可靠 | 交叉标注 + 一致性指标 |


---

## 总结

评估方法论是LLM系统工程的“地基”。**离线评估负责速度与诊断，在线评估负责真实效果验证**。统计显著性、置信区间、效应量与样本量是可靠结论的基础。对于LLM，还需要特别关注生成指标与人类偏好之间的差距，并加入安全性与成本护栏。最终，评估应当形成闭环：**定义目标 → 设计指标 → 实验验证 → 误差分析 → 迭代优化**。

当评估系统成熟后，团队才能稳定地回答：

1. 模型真的更好了吗？
2. 用户是否感知到了价值？
3. 风险是否处于可控范围？

这正是评估方法论的核心价值。

评估不是一次性工作，而是长期演进的工程能力。只有把评估嵌入研发与上线流程，才能持续保证模型与系统的可靠性。
同时应持续监测数据分布与用户行为的漂移，及时更新评估集与指标阈值。
当监控触发报警时，应执行回滚、降级或热修复策略。
这能确保评估结论与线上体验长期一致。
评估文化的建立是团队成熟度的标志。
持续改进永无止境。
